# -*- coding: utf-8 -*-
"""Updated CIFAR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Za0Dq1-0qczn9ZeKv-NiHylgFV9lEE8G
"""

import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split

import torchvision
from torchvision import datasets
from torchvision import transforms

import os
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
import pandas as pd
import seaborn as sn
import numpy as np

import random

from typing import List, Dict, Tuple

from tqdm.auto import tqdm

# Set seeds for reproducibility
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

# Define normalization values for CIFAR-10 dataset
mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]

# Define data augmentation and transformation pipeline for training
data_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
    transforms.RandomErasing(p=0.75, scale=(0.02, 0.1), value=1.0, inplace=False)
])

# test/validation data
test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

# Load the full training data
full_train_data = datasets.CIFAR10(
    root="data",
    train=True,
    download=True,
    transform=data_transform
)

# Setup testing data with test transform
test_data = datasets.CIFAR10(
    root="data",
    train=False,
    download=True,
    transform=test_transform
)

# Split the training data into training and validation sets (80% train, 20% validation)
train_size = int(0.8 * len(full_train_data))
val_size = len(full_train_data) - train_size
train_data, val_data = random_split(full_train_data, [train_size, val_size])

val_data = datasets.CIFAR10(
    root="data",
    train=True,
    download=True,
    transform=test_transform
)
val_data = torch.utils.data.Subset(val_data, range(train_size, len(full_train_data)))

class_names = full_train_data.classes

class_names

# Display a few random images from the training set
def show_random_images(dataset, num_images=16):
    rand_idx = random.sample(range(len(dataset)), k=num_images)

    plt.figure(figsize=(10, 10))

    for i, idx in enumerate(rand_idx):
        img, label = dataset[idx]
        img = (img - img.min()) / (img.max() - img.min())
        img_class = class_names[label]

        plt.subplot(4, 4, i+1)
        plt.imshow(img.permute(1, 2, 0))
        plt.title(f"Class: {img_class}", fontsize=10)
        plt.axis(False)

    plt.tight_layout()
    plt.show()

#show_random_images(train_data)

# Setup data loaders
NUM_WORKERS = os.cpu_count()
BATCH_SIZE = 256

train_dataloader = DataLoader(
    dataset=train_data,
    batch_size=BATCH_SIZE,
    num_workers=NUM_WORKERS,
    shuffle=True
)

val_dataloader = DataLoader(
    dataset=val_data,
    batch_size=BATCH_SIZE,
    num_workers=NUM_WORKERS,
    shuffle=False
)

test_dataloader = DataLoader(
    dataset=test_data,
    batch_size=BATCH_SIZE,
    num_workers=NUM_WORKERS,
    shuffle=False
)

def train_step(model: nn.Module,
               dataloader: torch.utils.data.DataLoader,
               loss_fn: nn.Module,
               optimizer: torch.optim.Optimizer,
               device: torch.device,
               grad_clip: float = None):

    model.train()

    train_loss, train_acc = 0, 0

    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        y_pred = model(X)
        loss = loss_fn(y_pred, y)
        train_loss += loss.item()

        optimizer.zero_grad()
        loss.backward()

        if grad_clip:
            nn.utils.clip_grad_value_(model.parameters(), grad_clip)

        optimizer.step()

        # Calculate accuracy
        y_pred_class = torch.argmax(y_pred, dim=1)
        train_acc += (y_pred_class == y).sum().item() / len(y)

    train_loss /= len(dataloader)
    train_acc /= len(dataloader)

    return train_loss, train_acc

def eval_step(model: nn.Module,
              dataloader: torch.utils.data.DataLoader,
              loss_fn: nn.Module,
              device: torch.device):

    model.eval()

    val_loss, val_acc = 0, 0
    y_true = []
    y_pred = []

    with torch.inference_mode():
        for batch, (X, y) in enumerate(dataloader):
            X, y = X.to(device), y.to(device)

            val_pred_logits = model(X)
            loss = loss_fn(val_pred_logits, y)
            val_loss += loss.item()

            # Calculate accuracy
            val_pred_labels = torch.argmax(val_pred_logits, dim=1)
            val_acc += (val_pred_labels == y).sum().item() / len(y)

            y_true.extend(y.cpu().numpy())
            y_pred.extend(val_pred_labels.cpu().numpy())

    val_loss /= len(dataloader)
    val_acc /= len(dataloader)

    # Calculate precision, recall, f1-score
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='weighted', zero_division=0)

    metrics = {
        "loss": val_loss,
        "acc": val_acc,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

    return metrics, y_true, y_pred

class EarlyStopping:
    """Early stopping to prevent overfitting"""
    def __init__(self, patience=5, min_delta=0.001, path='best_model.pt'):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False
        self.path = path

    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            # Save the best model
            torch.save(model.state_dict(), self.path)
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                print(f"Early stopping triggered after {self.patience} epochs without improvement")

        return self.early_stop

def train(model: torch.nn.Module,
          train_dataloader: torch.utils.data.DataLoader,
          val_dataloader: torch.utils.data.DataLoader,
          test_dataloader: torch.utils.data.DataLoader,
          optimizer: torch.optim.Optimizer,
          scheduler: torch.optim.lr_scheduler,
          device: torch.device,
          grad_clip: float = None,
          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),
          epochs: int = 10,
          model_save_path: str = 'best_model.pt'):

    # Initialize results dictionary
    results = {
        "train_loss": [],
        "train_acc": [],
        "val_loss": [],
        "val_acc": [],
        "val_precision": [],
        "val_recall": [],
        "val_f1": [],
        "test_metrics": None,
        "best_epoch": 0
    }

    # Initialize early stopping
    early_stopping = EarlyStopping(patience=5, path=model_save_path)

    # Training loop
    for epoch in tqdm(range(epochs)):
        train_loss, train_acc = train_step(
            model=model,
            dataloader=train_dataloader,
            loss_fn=loss_fn,
            optimizer=optimizer,
            device=device,
            grad_clip=grad_clip
        )

        # Validation step
        val_metrics, _, _ = eval_step(
            model=model,
            dataloader=val_dataloader,
            loss_fn=loss_fn,
            device=device
        )

        # Update learning rate scheduler based on validation loss
        if scheduler is not None:
            scheduler.step(val_metrics["loss"])

        # Store results
        results["train_loss"].append(train_loss)
        results["train_acc"].append(train_acc)
        results["val_loss"].append(val_metrics["loss"])
        results["val_acc"].append(val_metrics["acc"])
        results["val_precision"].append(val_metrics["precision"])
        results["val_recall"].append(val_metrics["recall"])
        results["val_f1"].append(val_metrics["f1"])

        # Print results
        print(
            f"Epoch: {epoch+1}/{epochs} | "
            f"Train Loss: {train_loss:.4f} | "
            f"Train Acc: {train_acc:.4f} | "
            f"Val Loss: {val_metrics['loss']:.4f} | "
            f"Val Acc: {val_metrics['acc']:.4f} | "
            f"Precision: {val_metrics['precision']:.4f} | "
            f"Recall: {val_metrics['recall']:.4f} | "
            f"F1: {val_metrics['f1']:.4f}"
        )

        # Check early stopping
        if early_stopping(val_metrics["loss"], model):
            results["best_epoch"] = epoch + 1 - early_stopping.patience
            print(f"Training stopped early at epoch {epoch+1}")
            break

    # Load the best model for final evaluation
    model.load_state_dict(torch.load(model_save_path))

    # Final evaluation based on test set
    test_metrics, y_true, y_pred = eval_step(
        model=model,
        dataloader=test_dataloader,
        loss_fn=loss_fn,
        device=device
    )
    results["test_metrics"] = test_metrics

    print(f"\nTest Metrics:")
    print(f"Loss: {test_metrics['loss']:.4f}")
    print(f"Accuracy: {test_metrics['acc']:.4f}")
    print(f"Precision: {test_metrics['precision']:.4f}")
    print(f"Recall: {test_metrics['recall']:.4f}")
    print(f"F1 Score: {test_metrics['f1']:.4f}")

    # Confusion matrix
    confusion_mat = confusion_matrix(y_true, y_pred)
    confusion_df = pd.DataFrame(
        confusion_mat / np.sum(confusion_mat, axis=1)[:, np.newaxis],
        index=class_names,
        columns=class_names
    )

    plt.figure(figsize=(12, 7))
    sn.heatmap(confusion_df, annot=True, cmap='Blues')
    plt.title('Normalized Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()

    return results, confusion_mat

def plot_metrics(results: Dict[str, List[float]]):
    """Plot training and validation metrics over epochs"""
    epochs = range(1, len(results['train_loss']) + 1)

    plt.figure(figsize=(15, 15))

    # Plot loss
    plt.subplot(2, 2, 1)
    plt.plot(epochs, results['train_loss'], 'b-', label='Training Loss')
    plt.plot(epochs, results['val_loss'], 'r-', label='Validation Loss')
    plt.title('Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.legend()

    # Plot accuracy
    plt.subplot(2, 2, 2)
    plt.plot(epochs, results['train_acc'], 'b-', label='Training Accuracy')
    plt.plot(epochs, results['val_acc'], 'r-', label='Validation Accuracy')
    plt.title('Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.grid(True)
    plt.legend()

    # Plot precision and recall
    plt.subplot(2, 2, 3)
    plt.plot(epochs, results['val_precision'], 'g-', label='Precision')
    plt.plot(epochs, results['val_recall'], 'y-', label='Recall')
    plt.title('Precision and Recall')
    plt.xlabel('Epochs')
    plt.ylabel('Score')
    plt.grid(True)
    plt.legend()

    # Plot F1 score
    plt.subplot(2, 2, 4)
    plt.plot(epochs, results['val_f1'], 'purple', label='F1 Score')
    plt.title('F1 Score')
    plt.xlabel('Epochs')
    plt.ylabel('Score')
    plt.grid(True)
    plt.legend()

    plt.tight_layout()
    plt.show()

class ResNet9(nn.Module):
    def conv_block(self, input_channels, output_channels, use_pool=False):
        layers = [
            nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(output_channels),
            nn.ReLU(inplace=True)
        ]
        if use_pool:
            layers.append(nn.MaxPool2d(2))
        return nn.Sequential(*layers)

    def __init__(self, input_channels, number_classes):
        super().__init__()

        self.conv1 = self.conv_block(input_channels, 64)
        self.conv2 = self.conv_block(64, 128, use_pool=True)
        self.residual1 = nn.Sequential(self.conv_block(128, 128), self.conv_block(128, 128))

        self.conv3 = self.conv_block(128, 256, use_pool=True)
        self.conv4 = self.conv_block(256, 512, use_pool=True)
        self.residual2 = nn.Sequential(self.conv_block(512, 512), self.conv_block(512, 512))

        self.classifier = nn.Sequential(
            nn.MaxPool2d(4),
            nn.Flatten(),
            nn.Linear(512, number_classes)
        )

    def forward(self, xb):
        layer1 = self.conv1(xb)
        layer2 = self.conv2(layer1)
        residual1 = self.residual1(layer2) + layer2
        layer3 = self.conv3(residual1)
        layer4 = self.conv4(layer3)
        residual2 = self.residual2(layer4) + layer4
        class_output = self.classifier(residual2)
        return class_output

def export_model(model, model_path, device):
    """Export the model in multiple formats"""

    dummy_input = torch.zeros(1, 3, 32, 32).to(device)

    traced_model = torch.jit.trace(model, dummy_input)
    torch.jit.save(traced_model, model_path)
    print(f"Model exported to {model_path}")

    # Save PyTorch state dict for PyTorch compatibility
    torch.save(model.state_dict(), model_path.replace('.pt', '_state_dict.pt'))
    print(f"Model state dictionary saved to {model_path.replace('.pt', '_state_dict.pt')}")

    torch.save(model, model_path.replace('.pt', '_full.pt'))
    print(f"Full model saved to {model_path.replace('.pt', '_full.pt')}")

def run_training():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    NUM_EPOCHS = 30
    learning_rate = 0.001
    weight_decay = 35e-5
    grad_clip = 0.0001
    model_save_path = 'best_cifar10_model.pt'

    model = ResNet9(3, 10).to(device)

    # Define loss function and optimizer
    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay
    )

    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer=optimizer,
        mode='min',
        factor=0.3,
        patience=3,
        threshold=0.09
    )

    from timeit import default_timer as timer
    start_time = timer()

    # Train the model
    results, confusion_mat = train(
        model=model,
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        test_dataloader=test_dataloader,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        grad_clip=grad_clip,
        loss_fn=loss_fn,
        epochs=NUM_EPOCHS,
        model_save_path=model_save_path
    )

    end_time = timer()
    print(f"Total training time: {end_time-start_time:.3f} seconds")

    plot_metrics(results)

    # Export the model
    # export_model(model, 'cifar1_10.pt', device)

    return model, results, confusion_mat

model = ResNet9(3, 10).to("cuda" if torch.cuda.is_available() else "cpu")
run_training()

from PIL import Image

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

device = "cuda" if torch.cuda.is_available() else "cpu"

model_path = '/content/best_cifar10_model.pt'  # Path of the trained model
model = ResNet9(3, 10)
model.load_state_dict(torch.load(model_path))
model.to(device)
model.eval()

# Test model with image
def predict_image(image_path):
    """
    Predict the class of a single image and display it with the result
    """
    image = Image.open(image_path).convert('RGB')
    plt.figure(figsize=(6, 6))
    plt.imshow(image)

    image_tensor = test_transform(image.resize((32, 32)))

    image_tensor = image_tensor.unsqueeze(0).to(device)

    with torch.no_grad():
        outputs = model(image_tensor)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        confidence, prediction = torch.max(probabilities, 1)

    predicted_class = class_names[prediction.item()]
    confidence_value = confidence.item() * 100

    values, indices = torch.topk(probabilities, 3)
    top_predictions = [(class_names[idx.item()], val.item() * 100) for idx, val in zip(indices[0], values[0])]

    plt.title(f"Prediction: {predicted_class} ({confidence_value:.2f}%)", fontsize=14)
    plt.axis('off')
    plt.show()

    print("Top 3 predictions:")
    for class_name, prob in top_predictions:
        print(f"{class_name}: {prob:.2f}%")

    return {
        "predicted_class": predicted_class,
        "confidence": confidence_value,
        "top_predictions": top_predictions
    }

# Example usage
image_path = "/content/frog-macro-amphibian-green-70083.jpeg"  # Image Path
result = predict_image(image_path)
result